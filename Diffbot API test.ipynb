{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 15%|█▍        | 149/997 [03:49<21:47,  1.54s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot process 'Q50429848'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panchenko/kb2vec/converter.py\", line 63, in wikidataid2wikipedia\n",
      "    entity = self._client.get(wikidata_q_id, load=True)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 139, in get\n",
      "    entity.load()\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/entity.py\", line 239, in load\n",
      "    result = self.client.request(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 193, in request\n",
      "    response = self.opener.open(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 570, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 26%|██▌       | 259/997 [05:50<16:38,  1.35s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 468/997 [10:17<11:37,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no links to English Wiki found, but found 3 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 51%|█████▏    | 511/997 [11:21<10:48,  1.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 58%|█████▊    | 581/997 [12:44<09:07,  1.32s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 3 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 86%|████████▌ | 855/997 [18:11<03:01,  1.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: no links to English Wiki found, but found 2 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n",
      "Warning: no links to English Wiki found, but found 1 links to other Wikis\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|█████████▍| 941/997 [20:36<01:13,  1.31s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: cannot process 'Q50430798'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panchenko/kb2vec/converter.py\", line 63, in wikidataid2wikipedia\n",
      "    entity = self._client.get(wikidata_q_id, load=True)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 139, in get\n",
      "    entity.load()\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/entity.py\", line 239, in load\n",
      "    result = self.client.request(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 193, in request\n",
      "    response = self.opener.open(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 570, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "\n",
      "Warning: cannot process 'Q50429848'\n",
      "Traceback (most recent call last):\n",
      "  File \"/home/panchenko/kb2vec/converter.py\", line 63, in wikidataid2wikipedia\n",
      "    entity = self._client.get(wikidata_q_id, load=True)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 139, in get\n",
      "    entity.load()\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/entity.py\", line 239, in load\n",
      "    result = self.client.request(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/site-packages/wikidata/client.py\", line 193, in request\n",
      "    response = self.opener.open(url)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 532, in open\n",
      "    response = meth(req, response)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 642, in http_response\n",
      "    'http', request, response, code, msg, hdrs)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 570, in error\n",
      "    return self._call_chain(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 504, in _call_chain\n",
      "    result = func(*args)\n",
      "  File \"/home/panchenko/anaconda/lib/python3.6/urllib/request.py\", line 650, in http_error_default\n",
      "    raise HTTPError(req.full_url, code, msg, hdrs, fp)\n",
      "urllib.error.HTTPError: HTTP Error 404: Not Found\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 997/997 [21:20<00:00,  1.28s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "103227\n"
     ]
    }
   ],
   "source": [
    "from linkers.context_aware import ContextAwareLinker \n",
    "from collections import defaultdict\n",
    "from candidate import Candidate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from candidate import Phrase, make_phrases\n",
    "import re\n",
    "from pandas import read_csv \n",
    "\n",
    "       \n",
    "class DenseLinker(ContextAwareLinker):\n",
    "    def foo(self):\n",
    "        pass\n",
    "    \n",
    "    def _vectorize_texts(self, texts):\n",
    "        # encode all these using sentence embeddings as well and/or average word2vec\n",
    "\n",
    "        # load a gensim model\n",
    "\n",
    "        # tokenize the words\n",
    "        # average the words\n",
    "\n",
    "        return \n",
    " \n",
    "class SparseLiker(ContextAwareLinker):\n",
    "    def __init__(self, tfidf=True):\n",
    "        ContextAwareLinker.__init__(self)\n",
    "        self._tfidf = tfidf\n",
    "          \n",
    "    def train(self, dataset_fpaths):\n",
    "        phrases = self._dataset2phrases(dataset_fpaths)\n",
    "        phrase2candidates = self._train(phrases)\n",
    "    \n",
    "        candidates = set()\n",
    "        for phrase in phrase2candidates:\n",
    "            for candidate in phrase2candidates[phrase]:\n",
    "                candidates.add(candidate)\n",
    "        print(len(candidates))\n",
    "    \n",
    "        return candidates \n",
    "    \n",
    "    def _train(self, phrases):\n",
    "        candidates = self.get_candidates(phrases)\n",
    "        return candidates\n",
    "        # create a vectorizable dictionary from the candidates \n",
    "        \n",
    "        #vectorizer = TfidfVectorizer() if tfidf else CountVectorizer()\n",
    "        \n",
    "        \n",
    "    # to an intermetidate TrainableLinker class?\n",
    "    def _dataset2phrases(self, dataset_fpaths):\n",
    "        voc = set()\n",
    "        for dataset_fpath in dataset_fpaths:\n",
    "            df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "            for i, row in df.iterrows():\n",
    "                for target in str(row.targets).split(\",\"):\n",
    "                    voc.add(target.strip())\n",
    "            \n",
    "        return make_phrases(list(voc))\n",
    "        \n",
    "    def link(self, context, phrases):\n",
    "        #X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "    \n",
    "    \n",
    "# l = ContextAwareLinker()\n",
    "# c = l.get_candidates(make_dummy_phrases([\"San Francisco\"]))\n",
    "dataset_fpaths = [\"datasets/dbpedia.tsv\", \"datasets/kore50.tsv\", \"datasets/n3-reuters-128.tsv\"]\n",
    "\n",
    "sl = SparseLiker()\n",
    "c = sl.train(dataset_fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from candidate import make_dummy_phrases \n",
    "\n",
    "\n",
    "output_fpath = \"data/sf-candidates.txt\"\n",
    "re_newlines = re.compile(r\"[\\n\\r]+\")\n",
    "\n",
    "with codecs.open(output_fpath, \"w\", \"utf-8\") as c_f:\n",
    "    for phrase in c:\n",
    "        for candidate in c[phrase]:\n",
    "            text = candidate.text\n",
    "            c_f.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "                phrase.text,\n",
    "                candidate.name,\n",
    "                text.strip()))\n",
    "            \n",
    "print(output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "False\n",
      "vectors: datasets/103227/vectors\n",
      "vectorizer: datasets/103227/vectorizer\n",
      "candidate2index: datasets/103227/candidate2index.pkl\n",
      "params: datasets/103227/params.json\n"
     ]
    }
   ],
   "source": [
    "# from utils import ensure_dir \n",
    "from os.path import join \n",
    "from sklearn.externals import joblib\n",
    "import json \n",
    "import os\n",
    "\n",
    "\n",
    "def ensure_dir(f):\n",
    "    \"\"\" Make the directory. \"\"\"\n",
    "    \n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "        \n",
    "candidates = list(c)\n",
    "model_dir = \"datasets/103227/\"\n",
    "\n",
    "# loading ...\n",
    "# clf = joblib.load('filename.pkl') \n",
    "# with open('data.json', 'r') as fp:\n",
    "#     data = json.load(fp)\n",
    "\n",
    "\n",
    "# make self constants in constructor \n",
    "vectorizer_filename = \"vectorizer\"\n",
    "candidate2index_filename = \"candidate2index.pkl\"\n",
    "params_filename = \"params.json\"\n",
    "vectors_filename = \"vectors\"\n",
    "tfidf = True # instead self._params[\"tfidf\"] = True\n",
    "\n",
    "vectorizer_fpath = join(model_dir, vectorizer_filename)\n",
    "candidate2index_fpath = join(model_dir, candidate2index_filename)\n",
    "params_fpath = join(model_dir, params_filename) \n",
    "vectors_fpath = join(model_dir, vectors_filename)\n",
    "\n",
    "# prepare the data\n",
    "candidate2index = {}\n",
    "corpus = []\n",
    "for index, candidate in enumerate(candidates):\n",
    "    corpus.append(candidate.text)\n",
    "    candidate2index[candidate] = index\n",
    "    \n",
    "# get the vectors\n",
    "vectorizer = TfidfVectorizer() if tfidf else CountVectorizer()\n",
    "vectors = vectorizer.fit_transform(corpus)\n",
    "\n",
    "# save the results\n",
    "\n",
    "print(ensure_dir(model_dir)) # make it in the constructor \n",
    "\n",
    "joblib.dump(vectors, vectors_fpath)\n",
    "print(\"vectors:\", vectors_fpath)\n",
    "\n",
    "joblib.dump(vectorizer, vectorizer_fpath) \n",
    "print(\"vectorizer:\", vectorizer_fpath)\n",
    "\n",
    "joblib.dump(candidate2index, candidate2index_fpath)\n",
    "print(\"candidate2index:\", candidate2index_fpath)\n",
    "\n",
    "# not here but in constructor ...\n",
    "# with open(params_fpath, \"w\") as fp:\n",
    "#    json.dump(self._params, fp)\n",
    "print(\"params:\", params_fpath)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data/997-phrases.txt\n"
     ]
    }
   ],
   "source": [
    "import codecs\n",
    "\n",
    "output_fpath = \"data/997-phrases.txt\"\n",
    "\n",
    "with codecs.open(output_fpath, \"w\", \"utf-8\") as out:\n",
    "    for candidate in c:\n",
    "        out.write(\"{}\\t{}\\n\".format(candidate.name, candidate.text))\n",
    "\n",
    "print(output_fpath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "\n",
    "context = \"San Francisco said the visit would serve as a cornerstone for future interaction between players and coaches from the Nets and young Russians, with the aim of developing basketball in Russia, where the sport is a distant third in popularity behind soccer and hockey.\"\n",
    "phrases = \"San Francisco\"\n",
    "\n",
    "phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                   for phrase in phrases.split(\",\")]\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for phrase, candidate in bl.link(context, phrases):\n",
    "    print(phrase.text, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "from pandas import read_csv \n",
    "\n",
    "dataset_fpath = \"datasets/dbpedia.tsv\"\n",
    "\n",
    "df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                       for phrase in row.targets.split(\",\")]\n",
    "        \n",
    "    print(\"\\n\\n{}\\n\".format(row.context))\n",
    "    \n",
    "    for phrase, candidate in bl.link(row.context, phrases):\n",
    "        link = candidate.link if candidate else \"\"\n",
    "        print(phrase.text, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tqmd'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-236db676cd86>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtqmd\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tqmd'"
     ]
    }
   ],
   "source": [
    "import tqmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
