{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Parameters:\n",
      "-  tfidf: True\n",
      "- description: \n",
      "- num_phrases: 997\n",
      "- num_candidates: 103227\n",
      "- shape: [103227, 218026]\n",
      "Loaded in 7.33 sec.\n"
     ]
    }
   ],
   "source": [
    "from linkers.context_aware import ContextAwareLinker \n",
    "from collections import defaultdict\n",
    "from candidate import Candidate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from candidate import Phrase, make_phrases\n",
    "import re\n",
    "from pandas import read_csv \n",
    "from time import time\n",
    "from os.path import join\n",
    "from utils import ensure_dir\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "from os.path import exists\n",
    "\n",
    "\n",
    "class DenseLinker(ContextAwareLinker):\n",
    "    def foo(self):\n",
    "        pass\n",
    "    \n",
    "    def _vectorize_texts(self, texts):\n",
    "        # encode all these using sentence embeddings as well and/or average word2vec\n",
    "\n",
    "        # load a gensim model\n",
    "\n",
    "        # tokenize the words\n",
    "        # average the words\n",
    "\n",
    "        return \n",
    " \n",
    "\n",
    "# ToDo: save also directly the phrase2index file for faster classifications\n",
    "\n",
    "class SparseLiker(ContextAwareLinker):\n",
    "    def __init__(self, model_dir, tfidf=True, description=\"\"):\n",
    "        ContextAwareLinker.__init__(self)\n",
    "        self._params = {}\n",
    "        self._params[\"tfidf\"] = tfidf\n",
    "        self._params[\"description\"] = description\n",
    "        \n",
    "        vectorizer_filename = \"vectorizer.pkl\"\n",
    "        candidate2index_filename = \"candidate2index.pkl\"\n",
    "        params_filename = \"params.json\"\n",
    "        vectors_filename = \"vectors.pkl\"\n",
    "        phrase2candidates_filename = \"phrase2candidate.pkl\"\n",
    "        \n",
    "        self._vectorizer_fpath = join(model_dir, vectorizer_filename)\n",
    "        self._candidate2index_fpath = join(model_dir, candidate2index_filename)\n",
    "        self._params_fpath = join(model_dir, params_filename) \n",
    "        self._vectors_fpath = join(model_dir, vectors_filename)\n",
    "        self._phrase2candidates_fpath = join(model_dir, phrase2candidates_filename)\n",
    "\n",
    "        self._load(model_dir) # using the defined paths\n",
    "        \n",
    "    def _load(self, model_dir):\n",
    "        tic = time()\n",
    "        ensure_dir(model_dir) \n",
    "\n",
    "        if exists(self._params_fpath):\n",
    "            with open(self._params_fpath, \"r\") as fp:\n",
    "                self._params = json.load(fp)\n",
    "            print(\"Parameters:\\n- \", \"\\n- \".join(\"{}: {}\".format(p, self._params[p]) for p in self._params))\n",
    "         \n",
    "        if exists(self._phrase2candidates_fpath):\n",
    "            self._phrase2candidates = joblib.load(self._phrase2candidates_fpath) \n",
    "        \n",
    "        if exists(self._candidate2index_fpath):\n",
    "            self._candidate2index = joblib.load(self._candidate2index_fpath) \n",
    "        \n",
    "        if exists(self._vectorizer_fpath):\n",
    "            self._vectorizer = joblib.load(self._vectorizer_fpath) \n",
    "        \n",
    "        if exists(self._vectors_fpath):\n",
    "            self._vectors = joblib.load(self._vectors_fpath)\n",
    "            \n",
    "        print(\"Loaded in {:.2f} sec.\".format(time()-tic))\n",
    "        \n",
    "    def train(self, dataset_fpaths):\n",
    "        tic = time()\n",
    "        print(\"Training...\")\n",
    "        phrases = self._dataset2phrases(dataset_fpaths)\n",
    "        self._train(phrases)\n",
    "        print(\"Training is done in {:.2f} sec.\".format(time()-tic))\n",
    "        \n",
    "    def _train(self, phrases):\n",
    "        self._params[\"num_phrases\"] = len(phrases)\n",
    "        print(\"Number of phrases:\", len(phrases))\n",
    "        \n",
    "        # get the candidates\n",
    "#         self._phrase2candidates = self.get_candidates(phrases)\n",
    "#         candidates = set()\n",
    "#         for phrase in _phrase2candidates:\n",
    "#             for candidate in _phrase2candidates[phrase]:\n",
    "#                 candidates.add(candidate)\n",
    "#         print(\"Number of candidates:\", len(candidates))\n",
    "#         joblib.dump(self._phrase2candidate, self._phrase2candidate_fpath)\n",
    "#         print(\"Saved phrase2candidate:\", self._phrase2candidate_fpath)\n",
    "    \n",
    "        import pickle \n",
    "        candidates = pickle.load(open(\"datasets/103227-candidate-texts.pkl\",\"rb\"))\n",
    "        self._params[\"num_candidates\"] = len(candidates)\n",
    "        print(\"Number of candidates:\", len(candidates))\n",
    "         \n",
    "        \n",
    "        self._candidate2index = {}\n",
    "        corpus = []\n",
    "        for index, candidate in enumerate(candidates):\n",
    "            corpus.append(candidate.text)\n",
    "            self._candidate2index[candidate] = index\n",
    "\n",
    "        joblib.dump(self._candidate2index, self._candidate2index_fpath)\n",
    "        print(\"Saved candidate2index:\", self._candidate2index_fpath)\n",
    "            \n",
    "        self._vectorizer = TfidfVectorizer() if self._params[\"tfidf\"] else CountVectorizer()\n",
    "        self._vectors = self._vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        joblib.dump(self._vectorizer, self._vectorizer_fpath) \n",
    "        print(\"Saved vectorizer:\", self._vectorizer_fpath)\n",
    "\n",
    "        joblib.dump(self._vectors, self._vectors_fpath)\n",
    "        self._params[\"shape\"] = self._vectors.shape\n",
    "        print(\"Saved {} candidate feature matrix: {}\".format(self._vectors.shape, self._vectors_fpath))\n",
    "\n",
    "        with open(self._params_fpath, \"w\") as fp:\n",
    "            json.dump(self._params, fp)\n",
    "        print(\"Saved params:\", self._params_fpath)\n",
    "        \n",
    "    # to an intermetidate TrainableLinker class?\n",
    "    def _dataset2phrases(self, dataset_fpaths):\n",
    "        voc = set()\n",
    "        for dataset_fpath in dataset_fpaths:\n",
    "            df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "            for i, row in df.iterrows():\n",
    "                for target in str(row.targets).split(\",\"):\n",
    "                    voc.add(target.strip())\n",
    "            \n",
    "        return make_phrases(list(voc))\n",
    "        \n",
    "    def link(self, context, phrases):\n",
    "        #X_train_counts = count_vect.fit_transform(twenty_train.data)\n",
    "        pass\n",
    "    \n",
    "dataset_fpaths = [\"datasets/dbpedia.tsv\", \n",
    "                  \"datasets/kore50.tsv\", \"datasets/n3-reuters-128.tsv\"]\n",
    "\n",
    "sl = SparseLiker(\"data/test2\")\n",
    "# candidates = sl.train(dataset_fpaths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "from candidate import make_dummy_phrases \n",
    "\n",
    "\n",
    "output_fpath = \"data/sf-candidates.txt\"\n",
    "re_newlines = re.compile(r\"[\\n\\r]+\")\n",
    "\n",
    "with codecs.open(output_fpath, \"w\", \"utf-8\") as c_f:\n",
    "    for phrase in c:\n",
    "        for candidate in c[phrase]:\n",
    "            text = candidate.text\n",
    "            c_f.write(\"{}\\t{}\\t{}\\n\".format(\n",
    "                phrase.text,\n",
    "                candidate.name,\n",
    "                text.strip()))\n",
    "            \n",
    "print(output_fpath)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from utils import ensure_dir \n",
    "from os.path import join \n",
    "import json \n",
    "import os\n",
    "\n",
    "\n",
    "def ensure_dir(f):\n",
    "    \"\"\" Make the directory. \"\"\"\n",
    "    \n",
    "    if not os.path.exists(f):\n",
    "        os.makedirs(f)\n",
    "        return True\n",
    "    else:\n",
    "        return False\n",
    "\n",
    "        \n",
    "candidates = list(c)\n",
    "# model_dir = \"datasets/103227/\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "c?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "output_fpath = \"data/997-phrases.txt\"\n",
    "\n",
    "with codecs.open(output_fpath, \"w\", \"utf-8\") as out:\n",
    "    for candidate in c:\n",
    "        out.write(\"{}\\t{}\\n\".format(candidate.name, candidate.text))\n",
    "\n",
    "print(output_fpath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "\n",
    "context = \"San Francisco said the visit would serve as a cornerstone for future interaction between players and coaches from the Nets and young Russians, with the aim of developing basketball in Russia, where the sport is a distant third in popularity behind soccer and hockey.\"\n",
    "phrases = \"San Francisco\"\n",
    "\n",
    "phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                   for phrase in phrases.split(\",\")]\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for phrase, candidate in bl.link(context, phrases):\n",
    "    print(phrase.text, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "from pandas import read_csv \n",
    "\n",
    "dataset_fpath = \"datasets/dbpedia.tsv\"\n",
    "\n",
    "df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                       for phrase in row.targets.split(\",\")]\n",
    "        \n",
    "    print(\"\\n\\n{}\\n\".format(row.context))\n",
    "    \n",
    "    for phrase, candidate in bl.link(row.context, phrases):\n",
    "        link = candidate.link if candidate else \"\"\n",
    "        print(phrase.text, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
