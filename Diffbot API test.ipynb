{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unexpected indent (<ipython-input-86-257f144a4649>, line 150)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-86-257f144a4649>\"\u001b[0;36m, line \u001b[0;32m150\u001b[0m\n\u001b[0;31m    def link(self, context, phrases):\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unexpected indent\n"
     ]
    }
   ],
   "source": [
    "from linkers.context_aware import ContextAwareLinker \n",
    "from collections import defaultdict\n",
    "from candidate import Candidate\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "from candidate import Phrase, make_phrases\n",
    "import re\n",
    "from pandas import read_csv \n",
    "from time import time\n",
    "from os.path import join\n",
    "from utils import ensure_dir\n",
    "from sklearn.externals import joblib\n",
    "import json\n",
    "from os.path import exists\n",
    "import codecs \n",
    "from numpy import dot, argmax\n",
    "\n",
    "\n",
    "class DenseLinker(ContextAwareLinker):\n",
    "    def foo(self):\n",
    "        pass\n",
    "    \n",
    "    def _vectorize_texts(self, texts):\n",
    "        # encode all these using sentence embeddings as well and/or average word2vec\n",
    "\n",
    "        # load a gensim model\n",
    "\n",
    "        # tokenize the words\n",
    "        # average the words\n",
    "\n",
    "        return \n",
    " \n",
    "\n",
    "# ToDo: save also directly the phrase2index file for faster classifications\n",
    "\n",
    "class SparseLiker(ContextAwareLinker):\n",
    "    def __init__(self, model_dir, tfidf=True, description=\"\"):\n",
    "        ContextAwareLinker.__init__(self)\n",
    "        self._params = {}\n",
    "        self._params[\"tfidf\"] = tfidf\n",
    "        self._params[\"description\"] = description\n",
    "        \n",
    "        vectorizer_filename = \"vectorizer.pkl\"\n",
    "        candidate2index_filename = \"candidate2index.pkl\"\n",
    "        params_filename = \"params.json\"\n",
    "        vectors_filename = \"vectors.pkl\"\n",
    "        phrase2candidates_filename = \"phrase2candidates.pkl\"\n",
    "        phrases_filename = \"phrases.txt\"\n",
    "        candidates_filename = \"candidates.txt\"\n",
    "        \n",
    "        self._vectorizer_fpath = join(model_dir, vectorizer_filename)\n",
    "        self._candidate2index_fpath = join(model_dir, candidate2index_filename)\n",
    "        self._params_fpath = join(model_dir, params_filename) \n",
    "        self._vectors_fpath = join(model_dir, vectors_filename)\n",
    "        self._phrase2candidates_fpath = join(model_dir, phrase2candidates_filename)\n",
    "        self._phrases_fpath = join(model_dir, phrases_filename)\n",
    "        self._candidates_fpath = join(model_dir, candidates_filename)\n",
    "        self._load(model_dir) # using the defined paths\n",
    "        \n",
    "    def _load(self, model_dir):\n",
    "        tic = time()\n",
    "        ensure_dir(model_dir) \n",
    "\n",
    "        if exists(self._params_fpath):\n",
    "            with open(self._params_fpath, \"r\") as fp:\n",
    "                self._params = json.load(fp)\n",
    "            print(\"Parameters:\\n- \", \"\\n- \".join(\"{}: {}\".format(p, self._params[p]) for p in self._params))\n",
    "         \n",
    "        if exists(self._phrase2candidates_fpath):\n",
    "            self._phrase2candidates = joblib.load(self._phrase2candidates_fpath) \n",
    "        \n",
    "        if exists(self._candidate2index_fpath):\n",
    "            self._candidate2index = joblib.load(self._candidate2index_fpath) \n",
    "        \n",
    "        if exists(self._vectorizer_fpath):\n",
    "            self._vectorizer = joblib.load(self._vectorizer_fpath) \n",
    "        \n",
    "        if exists(self._vectors_fpath):\n",
    "            self._vectors = joblib.load(self._vectors_fpath)\n",
    "            \n",
    "        print(\"Loaded in {:.2f} sec.\".format(time()-tic))\n",
    "        \n",
    "    def train(self, dataset_fpaths):\n",
    "        tic = time()\n",
    "        print(\"Training...\")\n",
    "        phrases = self._dataset2phrases(dataset_fpaths) \n",
    "        self._train(phrases)\n",
    "        print(\"Training is done in {:.2f} sec.\".format(time()-tic))\n",
    "        \n",
    "    def _train(self, phrases):\n",
    "             \n",
    "        with codecs.open(self._phrases_fpath, \"w\", \"utf-8\") as out:\n",
    "            for phrase in phrases: out.write(\"{}\\n\".format(phrase.text))\n",
    "        print(\"Saved phrases:\", self._phrases_fpath)                                      \n",
    "                                      \n",
    "        self._params[\"num_phrases\"] = len(phrases)\n",
    "        print(\"Number of phrases:\", len(phrases))\n",
    "        \n",
    "        # get the candidates\n",
    "#         self._phrase2candidates = self.get_candidates(phrases)\n",
    "        self._phrase2candidates = joblib.load(self._phrase2candidates_fpath) \n",
    "\n",
    "        candidates = set()\n",
    "        for phrase in self._phrase2candidates:\n",
    "            for candidate in self._phrase2candidates[phrase]:\n",
    "                candidates.add(candidate)\n",
    "        print(\"Number of candidates:\", len(candidates))\n",
    "        joblib.dump(self._phrase2candidates, self._phrase2candidates_fpath)\n",
    "        print(\"Saved phrase2candidate:\", self._phrase2candidates_fpath)\n",
    "     \n",
    "        with codecs.open(self._candidates_fpath, \"w\", \"utf-8\") as out:\n",
    "            for candidate in candidates: out.write(\"{}\\n\".format(candidate.name))\n",
    "        print(\"Saved candidates:\", self._candidates_fpath)\n",
    "\n",
    "        self._candidate2index = {}\n",
    "        corpus = []\n",
    "        for index, candidate in enumerate(candidates):\n",
    "            corpus.append(candidate.text)\n",
    "            self._candidate2index[candidate] = index\n",
    "\n",
    "        joblib.dump(self._candidate2index, self._candidate2index_fpath)\n",
    "        print(\"Saved candidate2index:\", self._candidate2index_fpath)\n",
    "            \n",
    "        self._vectorizer = TfidfVectorizer() if self._params[\"tfidf\"] else CountVectorizer()\n",
    "        self._vectors = self._vectorizer.fit_transform(corpus)\n",
    "        \n",
    "        joblib.dump(self._vectorizer, self._vectorizer_fpath) \n",
    "        print(\"Saved vectorizer:\", self._vectorizer_fpath)\n",
    "\n",
    "        joblib.dump(self._vectors, self._vectors_fpath)\n",
    "        self._params[\"shape\"] = self._vectors.shape\n",
    "        print(\"Saved {} candidate feature matrix: {}\".format(self._vectors.shape, self._vectors_fpath))\n",
    "\n",
    "        with open(self._params_fpath, \"w\") as fp:\n",
    "            json.dump(self._params, fp)\n",
    "        print(\"Saved params:\", self._params_fpath)\n",
    "        \n",
    "    # to an intermetidate TrainableLinker class?\n",
    "    def _dataset2phrases(self, dataset_fpaths):\n",
    "        voc = set()\n",
    "        for dataset_fpath in dataset_fpaths:\n",
    "            df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "            for i, row in df.iterrows():\n",
    "                for target in str(row.targets).split(\",\"):\n",
    "                    voc.add(target.strip())\n",
    "            \n",
    "        return make_phrases(list(voc))\n",
    "        \n",
    "    def link(self, context, phrases):       \n",
    "        linked_phrases = []\n",
    "        context_vector = self._vectorizer.transform([context])\n",
    "\n",
    "        for phrase in phrases:\n",
    "            candidate_indices = []    \n",
    "            if phrase in self._phrase2candidates:\n",
    "                # get the candidates\n",
    "                candidates = list(self._phrase2candidates[phrase])\n",
    "                indices = []\n",
    "                for candidate in candidates:\n",
    "                    # print(\".\", candidate.name)\n",
    "                    if candidate in self._candidate2index:\n",
    "                        indices.append(self._candidate2index[candidate])\n",
    "                    else:\n",
    "                        print(\"Warning: candidate '{}' is not indexed\".format(candidate))\n",
    "\n",
    "                candidate_vectors = self._vectors[ indices ]\n",
    "                print(\"Retrieved {} candidates for '{}'\".format(len(indices), phrase.text))\n",
    "\n",
    "                # rank the candidates\n",
    "                sims = dot(candidate_vectors, context_vector.T)\n",
    "                best_index = argmax(sims)\n",
    "                best_candidate = candidates[best_index]\n",
    "                best_candidate.score = sims.data[best_index]\n",
    "\n",
    "                linked_phrases.append( (phrase, best_candidate) )\n",
    "            else:\n",
    "                print(\"Warning: phrase '{}' is not found\".format(phrase))\n",
    "\n",
    "                linked_phrases.append( (phrase, Candidate()) )                \n",
    "\n",
    "dataset_fpaths = [\"datasets/dbpedia.tsv\", \n",
    "                  \"datasets/kore50.tsv\", \"datasets/n3-reuters-128.tsv\"]\n",
    "\n",
    "sl = SparseLiker(\"data/test99\")\n",
    "# sl.train(dataset_fpaths)\n",
    "context = \"Madonna is a great music  signer and lives near West Holywood in LA. adonna Louise Ciccone (/tʃɪˈkoʊni/; born August 16, 1958) is an American singer, songwriter, actress, and businesswoman. Referred to as the Queen of Pop since the 1980s, Madonna is known for pushing the boundaries of lyrical content in mainstream popular music, as well as visual imagery in music videos and on stage. She has also frequently reinvented both her music and image while maintaining autonomy within the recording industry. Besides sparking controversy, her works have bee \"\n",
    "phrases = [\"Madonna\"]\n",
    "sl.link(context, make_phrases(phrases))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import codecs\n",
    "\n",
    "output_fpath = \"data/997-phrases.txt\"\n",
    "\n",
    "with codecs.open(output_fpath, \"w\", \"utf-8\") as out:\n",
    "    for candidate in c:\n",
    "        out.write(\"{}\\t{}\\n\".format(candidate.name, candidate.text))\n",
    "\n",
    "print(output_fpath)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "\n",
    "context = \"San Francisco said the visit would serve as a cornerstone for future interaction between players and coaches from the Nets and young Russians, with the aim of developing basketball in Russia, where the sport is a distant third in popularity behind soccer and hockey.\"\n",
    "phrases = \"San Francisco\"\n",
    "\n",
    "phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                   for phrase in phrases.split(\",\")]\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for phrase, candidate in bl.link(context, phrases):\n",
    "    print(phrase.text, candidate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from linkers.baseline import BaselineLinker\n",
    "from candidate import Phrase\n",
    "from pandas import read_csv \n",
    "\n",
    "dataset_fpath = \"datasets/dbpedia.tsv\"\n",
    "\n",
    "df = read_csv(dataset_fpath, sep=\"\\t\", encoding=\"utf-8\")\n",
    "bl = BaselineLinker()\n",
    "\n",
    "for i, row in df.iterrows():\n",
    "    phrases =  [Phrase(phrase.strip(), 1, len(phrase.strip()), \"http://\" + phrase.strip())\n",
    "                       for phrase in row.targets.split(\",\")]\n",
    "        \n",
    "    print(\"\\n\\n{}\\n\".format(row.context))\n",
    "    \n",
    "    for phrase, candidate in bl.link(row.context, phrases):\n",
    "        link = candidate.link if candidate else \"\"\n",
    "        print(phrase.text, link)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqmd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
